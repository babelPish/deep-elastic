{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning of Video Representations using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 바벨피쉬 / 딥엘라스틱 : 파트 2 - 딥NLP [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1.Introduction\n",
    "    - 1.1. Why Unsupervised Learning?\n",
    "    - 1.2. Our Approach\n",
    "    - 1.3. Related Work\n",
    "* 2.Model Description\n",
    "    - 2.1. Long Short Term Memory\n",
    "    - 2.2. LSTM Autoencoder Model\n",
    "    - 2.3. LSTM Future Predictor Model\n",
    "    - 2.4. Conditional Decoder\n",
    "    - 2.5. A Composite Model\n",
    "* 3.Experiments\n",
    "    - 3.1. Datasets\n",
    "    - 3.2. Visualization and Qualitative Analysis\n",
    "    - 3.3. Action Recognition on UCF-101/HMDB-51\n",
    "    - 3.4. Comparison of Different Model Variants\n",
    "    - 3.5. Comparison with Other Action Recognition Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "[2] Unsupervised Learning of Video Representations using LSTMs slide - https://docs.google.com/presentation/d/1aF-HdZwR3jfHkyS_BL2jRYMM2dTFvT4zUBs4alN-GPs/edit#slide=id.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "* 1.1. Why Unsupervised Learning?\n",
    "* 1.2. Our Approach\n",
    "* 1.3. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Why Unsupervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The costly work of collecting more labelled data and the tedious work of doing more clever engineering can go a long way in solving particular problems, but this is ultimately unsatisfying as a machine learning solution. \n",
    "* This highlights the need for using unsupervised learning to find and represent structure in videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Our Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our model works as follows. \n",
    "    - The Encoder LSTM runs through a sequence of frames to come up with a representation. \n",
    "    - This representation is then decoded through another LSTM to produce a target sequence. \n",
    "    - We consider different choices of the target sequence.\n",
    "        - <font color=\"red\">One choice is to predict the same sequence as the input</font>. \n",
    "            - The motivation is similar to that of autoencoders \n",
    "                - – we wish to capture all that is needed to reproduce the input but at the same time go through the inductive biases imposed by the model. \n",
    "        - <font color=\"red\">Another option is to predict the future frames</font>. \n",
    "            - Here the motivation is to learn a representation that extracts all that is needed to extrapolate the motion and appearance beyond what has been observed. \n",
    "        - These two natural choices can also be combined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Description\n",
    "* 2.1. Long Short Term Memory\n",
    "* 2.2. LSTM Autoencoder Model\n",
    "* 2.3. LSTM Future Predictor Model\n",
    "* 2.4. Conditional Decoder\n",
    "* 2.5. A Composite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [7] 엘에스티엠 네트워크 이해하기 - http://roboticist.tistory.com/m/post/571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. LSTM Autoencoder Model\n",
    "* Why should this learn good features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [9] CS231n: Convolutional Neural Networks for Visual Recognition - Lecture 11 - Training ConvNets in practice : Data augmentation, transfer learning, Distributed training, CPU/GPU bottlenecks, Efficient convolutions - http://cs231n.stanford.edu/slides/winter1516_lecture11.pdf\n",
    "* [6] CS231n: Convolutional Neural Networks for Visual Recognition - Lecture 14 - ConvNets for videos Unsupervised learning - http://cs231n.stanford.edu/slides/winter1516_lecture14.pdf\n",
    "* [8] Deep Learning 이론과 실습 - https://wikidocs.net/3413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should this learn good features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The state of the encoder LSTM after the last input has been read is the representation of the input video. \n",
    "* The decoder LSTM is being asked to reconstruct back the input sequence from this representation. \n",
    "* In order to do so, the representation must retain information about the appearance of the objects and the background as well as the motion contained in the video. \n",
    "* However, an important question for any autoencoder-style model is what prevents it from learning an identity mapping and effectively copying the input to the output.\n",
    "* In that case all the information about the in- put would still be present but the representation will be no better than the input.\n",
    "* <font color=\"red\">There are two factors that control this behaviour</font>. \n",
    "    - First, the fact that there are only a fixed number of hidden units makes it unlikely that the model can learn trivial mappings for arbitrary length input sequences. \n",
    "    - Second, the same LSTM operation is used to decode the representation recursively. This means that the same dynamics must be applied on the representation at any stage of decoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. LSTM Future Predictor Model\n",
    "* Why should this learn good features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another natural unsupervised learning task for sequences is <font color=\"blue\">predicting the future</font>. \n",
    "* This is the approach used in language models for modeling sequences of words. \n",
    "* The design of the <font color=\"red\">Future Predictor Model</font> is same as that of the Autoencoder Model, except that the decoder LSTM in this case predicts frames of the video that come after the input sequence (Fig. 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  long sequence (future)\n",
    "* Ranzato et al. (2014) use a similar model but predict only the next frame at each time step.\n",
    "* This model, on the other hand, predicts a long sequence into the future.\n",
    "* Here again we can consider <font color=\"red\">two variants of the decoder </font>\n",
    "    - conditional and \n",
    "    - unconditioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should this learn good features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to predict the next few frames correctly, the model needs information about which objects and background are present and how they are moving so that the motion can be extrapolated. \n",
    "* <font color=\"red\">The hidden state coming out from the encoder will try to capture this information</font>. \n",
    "* <font color=\"blue\">Therefore, this state can be seen as a representation of the input sequence.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Conditional Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each of these two models, we can consider two possibilities \n",
    "    - one in which the decoder LSTM is <font color=\"red\">conditioned on the last generated frame</font> and \n",
    "    - the other in which it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. A Composite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This composite model tries to <font color=\"red\">overcome the shortcomings</font> that each model suffers on its own. \n",
    "\n",
    "#### memorization\n",
    "* A high-capacity autoencoder would suffer from the tendency to learn trivial representations that just memorize the inputs. \n",
    "* However, this memorization is not useful at all for predicting the future. Therefore, the composite model cannot just memorization. \n",
    "\n",
    "#### Forgetting \n",
    "* On the other hand, the future predictor suffers form the tendency to store information only about the last few frames since those are most important for predicting the future, \n",
    "    - i.e., in order to predict $v_t$, the frames {$v_{t−1}, . . . , v_{t−k}$} are much more important than $v_0$,for some small value of k.\n",
    "* Therefore the representation at the end of the encoder will have forgotten about a large part of the input. \n",
    "* But if we ask the model to also predict all of the input sequence, then it cannot just pay attention to the last few frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experiments\n",
    "* 3.1. Datasets\n",
    "* 3.2. Visualization and Qualitative Analysis\n",
    "* 3.3. Action Recognition on UCF-101/HMDB-51\n",
    "* 3.4. Comparison of Different Model Variants\n",
    "* 3.5. Comparison with Other Action Recognition Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Visualization and Qualitative Analysis\n",
    "* Experiments on MNIST\n",
    "* Experiments on Natural Image Patches\n",
    "* Generalization over time scales\n",
    "* Out-of-domain Inputs\n",
    "* Visualizing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Natural Image Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization over time scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-domain Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Action Recognition on UCF-101/HMDB-51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Comparison of Different Model Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Comparison with Other Action Recognition Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Unsupervised Learning of Video Representations using LSTMs /  ICML 2015 / arXiv:1502.04681  / Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov - http://arxiv.org/abs/1502.04681\n",
    "* [2] Unsupervised Learning of Video Representations using LSTMs\n",
    " slide - https://docs.google.com/presentation/d/1aF-HdZwR3jfHkyS_BL2jRYMM2dTFvT4zUBs4alN-GPs/edit#slide=id.p\n",
    "* [3] Topics in Computer Vision (CSC2523): Deep Learning in Computer Vision Winter 2016 - http://www.cs.utoronto.ca/~fidler/teaching/2015/CSC2523.html\n",
    "* [4] code(orignal) - http://www.cs.toronto.edu/~nitish/unsupervised_video/\n",
    "* [5] code (emansim/unsupervised-videos) - https://github.com/emansim/unsupervised-videos\n",
    "* [6] CS231n: Convolutional Neural Networks for Visual Recognition - Lecture 14 - ConvNets for videos Unsupervised learning - http://cs231n.stanford.edu/slides/winter1516_lecture14.pdf\n",
    "* [7] 엘에스티엠 네트워크 이해하기 - http://roboticist.tistory.com/m/post/571\n",
    "* [8] Deep Learning 이론과 실습 - https://wikidocs.net/3413\n",
    "* [9] CS231n: Convolutional Neural Networks for Visual Recognition - Lecture 11 - Training ConvNets in practice : Data augmentation, transfer learning, Distributed training, CPU/GPU bottlenecks, Efficient convolutions - http://cs231n.stanford.edu/slides/winter1516_lecture11.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
