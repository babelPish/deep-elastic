{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How NOT To Evaluate Your Dialogue System: \n",
    "An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 바벨피쉬 / 딥엘라스틱 : 파트 3 - 딥챗봇 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1 Introduction\n",
    "* 2 Related Work\n",
    "* 3 Evaluation Metrics\n",
    "    - 3.1 Word Overlap-based Metrics\n",
    "    - 3.2 Embedding-based Metrics\n",
    "* 4 End-to-End Dialogue Models\n",
    "    - 4.1 Retrieval Models\n",
    "    - 4.2 Generative Models\n",
    "    - 4.3 Conclusions from an Incomplete Analysis\n",
    "* 5 Human Correlation Analysis\n",
    "    - 5.1 Data collection\n",
    "    - 5.2 Survey Results\n",
    "    - 5.3 Qualitative Analysis\n",
    "* 6 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 \n",
    "* [2] 'How NOT To Evaluate Your Dialogue System'Paper's slide - http://cs.mcgill.ca/~rlowe1/dialogue_evaluation_mila2016.pdf\n",
    "* [3] Laurent Charlin's homepage - http://www.cs.toronto.edu/~lcharlin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We investigate evaluation metrics for end- to-end dialogue systems where supervised labels, such as task completion, are not available.\n",
    "* Recent works in end-to-end dialogue systems \n",
    "    - have adopted metrics from \n",
    "        - machine translation and \n",
    "        - text summarization \n",
    "    - to compare \n",
    "        - a model’s generated response \n",
    "        - to a single target response. \n",
    "* We show that these metrics \n",
    "    - correlate very weakly or \n",
    "    - not at all with human judgements of \n",
    "    - the response quality \n",
    "        - in both technical and non-technical domains. \n",
    "* We provide \n",
    "    - quantitative and qualitative results \n",
    "        - highlighting specific weaknesses in existing metrics, and \n",
    "    - provide recommendations \n",
    "        - for future development of \n",
    "            - better automatic evaluation metrics \n",
    "                - for dialogue systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant progress has been made in learning end-to-end systems \n",
    "* directly from large amounts of text data for a variety of natural language tasks, such as \n",
    "    - question answering (Weston et al., 2015), \n",
    "    - machine translation (Cho et al., 2014), and \n",
    "    - dialogue response generation systems (Sordoni et al., 2015), \n",
    "* in particular through the use of neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of dialogue systems, \n",
    "* an important challenge is to provide a <font color=\"red\">reliable evaluation of the learned systems</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised dialogue models VS  Unsupervised dialogue models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised dialogue models\n",
    "* Typically, evaluation is done using human-generated supervised signals, such as \n",
    "    - a task completion test or \n",
    "    - a user satisfaction score\n",
    "* We call models that are <font color=\"red\">trained to optimize for such supervised objectives</font> \n",
    "    - supervised dialogue models\n",
    "* While supervised models have historically been the method of choice,\n",
    "    - <font color=\"red\">supervised labels are difficult</font> \n",
    "        - to collect on a large scale \n",
    "            - due to the cost of human labour. \n",
    "    - Further, for free-form types of dialogues \n",
    "        - (e.g., chatbots), \n",
    "        - <font color=\"red\">the notion of task completion is ill-defined</font> \n",
    "            - since it may differ from one human user to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised dialogue models\n",
    "* Unsupervised dialogue models are receiving increased attention. \n",
    "* These models are typically \n",
    "    - trained (end-to-end) to \n",
    "    - predict the next utterance of a conversation, \n",
    "        - given several context utterances (Serban et al., 2015). \n",
    "    - This task is referred to as <font color=\"red\">response generation</font>. \n",
    "* However <font color=\"red\">automatically evaluating the quality of unsupervised models</font> \n",
    "    - remains an <font color=\"blue\">open question</font>. \n",
    "    - Automatic evaluation metrics would \n",
    "        - help accelerate the deployment of unsupervised dialogue systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faced with similar challenges, other natural language tasks have successfully developed automatic evaluation metrics.\n",
    "* machine translation\n",
    "    - BLEU (Papineni et al., 2002a)\n",
    "    - METEOR (Banerjee and Lavie, 2005) \n",
    "* automatic summarization\n",
    "    - ROUGE (Lin, 2004) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic evaluation metrics for dialogue response generation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the machine translation task appears similar to the dialogue response generation task, \n",
    "    - dialogue researchers have <font color=\"red\">adopted the same metrics</font> for evaluating the performance of their models.\n",
    "* However, the applicability of these methods has not been validated for dialogue-related tasks. \n",
    "    - A particular challenge in dialogues is \n",
    "        - the <font color=\"red\">significant diversity</font> \n",
    "            - in the space of valid responses to \n",
    "                - a given conversational context.\n",
    "    -  This is illustrated in Table 1, \n",
    "        - where two reasonable proposed responses are given to the context; \n",
    "        - however, these responses \n",
    "            - <font color=\"red\">do not share any words in common</font> and \n",
    "            - <font color=\"red\">do not have the same semantic meaning</font>.\n",
    "* We investigate several evaluation metrics for dialogue response generation systems, including both \n",
    "    - <font color=\"blue\">statistical word based similarity metrics</font> \n",
    "        - such as BLEU, METEOR, and ROUGE, and \n",
    "    - <font color=\"blue\">word-embedding based similarity metrics</font> \n",
    "        - derived from word embedding models such as Word2Vec (Mikolov et al., 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### applicability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We study the applicability of these metrics \n",
    "*  by using them to evaluate \n",
    "    - a <font color=\"red\">variety of end-to-end dialogue models</font>, including both \n",
    "        - <font color=\"blue\">retrieval models</font> such as \n",
    "            - the Dual Encoder (Lowe et al., 2015) and \n",
    "        - <font color=\"blue\">generative models</font> that incorporate some form of \n",
    "            - recurrent decoder (Serban et al., 2015). \n",
    "* We use these models \n",
    "    - to produce a proposed response \n",
    "        - given the context of the conversation and \n",
    "    - compare them to the ground-truth response \n",
    "        - (the actual next response) \n",
    "    - using the above metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating these models <font color=\"red\">with the embedding-based metrics</font>, \n",
    "* we find that even though some models significantly outperform others across several metrics and domains, <font color=\"red\">the metrics only very weakly correlate with human judgement</font>, \n",
    "    - as determined by human evaluation of the responses.\n",
    "\n",
    "We highlight the <font color=\"red\">shortcomings of these metrics</font> using: \n",
    "* a) a <font color=\"blue\">statistical analysis</font> of \n",
    "    - our survey’s results; \n",
    "* b) a <font color=\"blue\">qualitative analysis</font> of \n",
    "    - examples taken from our data; and \n",
    "* c) an <font color=\"blue\">exploration of the sensitivity</font> of \n",
    "    - the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation methods for supervised dialogue systems\n",
    "* PARADISE framework (Walker et al., 1997),\n",
    "* MeMo (Mo ̈ller et al., 2006) \n",
    "* Jokinen and McTear, 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model-independent\n",
    "* We <font color=\"red\">focus on metrics that are model-independent</font>, \n",
    "    - i.e. where the model generating the response does not also evaluate its quality; \n",
    "* thus, <font color=\"red\">we do not consider word perplexity</font>, \n",
    "    - although it has been used to evaluate unsupervised dialogue models (Serban et al., 2015).\n",
    "    - This is because it is not computed on a per-response basis, \n",
    "        - and cannot be computed for retrieval models. \n",
    "* Further, we only consider metrics \n",
    "    - that can be used to evaluate proposed responses \n",
    "        - against ground-truth responses, \n",
    "    - so <font color=\"red\">we do not consider retrieval-based metrics</font> such as \n",
    "        - <font color=\"blue\">recall</font>, \n",
    "            - which has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unsupervised dialogue systems \n",
    "* BLEU\n",
    "* Ritter et al. (2011b)\n",
    "* statistical machine translation (SMT) model\n",
    "* using a simple bag-of-words model\n",
    "* deltaBLEU (Galley et al., 2015b),\n",
    "* Galley et al (2015b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Evaluation Metrics\n",
    "* 3.1 Word Overlap-based Metrics\n",
    "* 3.2 Embedding-based Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Word Overlap-based Metrics\n",
    "* BLEU\n",
    "* METEOR\n",
    "* ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word-overlap\n",
    "We first consider metrics that evaluate the amount of word-overlap between the proposed response and the ground-truth response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [4] Chapter 8. Evaluation (Statistical Machine Translation) - http://www.statmt.org/book/slides/08-evaluation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU (Papineni et al., 2002a) analyzes the co-occurences of n-grams in the ground truth and the proposed responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/kantanmtworkshop-150501084851-conversion-gate01/95/eamt-workshop-2015-kantanmt-23-638.jpg?cb=1430470217\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [5] METEOR: Metric for Evaluation of Translation with Explicit Ordering An Automatic Metric for MT Evaluation with Improved Correlations with Human Judgments. - http://slideplayer.com/slide/7480005/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/meteor.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The METEOR metric (Banerjee and Lavie, 2005) was introduced to address several weaknesses in BLEU.\n",
    "* The alignment is based on exact token matching, followed by WordNet synonyms, stemmed tokens, and then paraphrases. \n",
    "* Given a set of alignments, the METEOR score is \n",
    "    - the harmonic mean of \n",
    "        - precision and recall \n",
    "            - between the proposed and ground truth sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [6] Text summarization - http://www.slideshare.net/kareemhashem/text-summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/textsummarization-130415121119-phpapp02/95/text-summarization-14-638.jpg?cb=1366027930\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE (Lin, 2004) is a set of evaluation metrics used for automatic summarization. \n",
    "* We consider ROUGE-L, which is a F-measure based on the Longest Common Subsequence (LCS) between a candidate and target sen- tence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Embedding-based Metrics\n",
    "* Greedy Matching\n",
    "* Embedding Average\n",
    "* Vector Extrema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to using word-overlap based metrics is to consider the meaning of each word as defined by a word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/w2v.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy matching is the one embedding-based metric that does <font color=\"red\">not compute sentence-level embeddings</font>. \n",
    "* Instead, given two sequences $r$ and $\\hat{r}$, each token $w ∈ r$ is greedily matched with a token $\\hat{w} ∈ \\hat{r}$ based on the cosine similarity of their word embeddings (ew), and the total score is then averaged across all words:\n",
    "    <img src=\"figures/cap4.png\" width=400 />\n",
    "* This formula is asymmetric, thus we must aver- age the greedy matching scores G in each direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding average metric <font color=\"red\">calculates sentence-level embeddings</font> using additive composition, a method for computing the meanings of phrases by averaging the vector representations of their constituent words\n",
    "* This method has been widely used in other domains, for example in textual similarity tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding average, $\\bar{e}$, is defined as the mean of the word embeddings of each token in a sentence r:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare a ground truth response $r$ and retrieved response $\\hat{r}$, \n",
    "* we compute the cosine similarity between their respective sentence level embeddings: \n",
    "    - EA := cos($\\bar{e}_{r}$ , $\\bar{e}_{\\hat{r}}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Extrema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to calculate sentence-level embeddings is using vector extrema (Forgues et al., 2014). \n",
    "* For each dimension of the word vectors, \n",
    "    - take the most extreme value amongst \n",
    "        - all word vectors in the sentence, and \n",
    "    - use that value in the sentence-level embedding:\n",
    "    <img src=\"figures/cap6.png\"  />\n",
    "        - where $d$ indexes the dimensions of a vector; $e_{wd}$ is the $d$’th dimensions of $ew$ ($w$’s embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 End-to-End Dialogue Models\n",
    "* 4.1 Retrieval Models\n",
    "* 4.2 Generative Models\n",
    "* 4.3 Conclusions from an Incomplete Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Retrieval Models\n",
    "* TF-IDF\n",
    "* Dual Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking or retrieval models for dialogue systems are typically evaluated based on whether they can retrieve the correct response <font color=\"red\">from a corpus of pre-defined responses</font>, which includes the ground truth response to the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/tfidf.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 \n",
    "* [7] DEEP LEARNING FOR CHATBOTS, PART 2 – IMPLEMENTING A RETRIEVAL-BASED MODEL IN TENSORFLOW - http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/\n",
    "* [8] Dual LSTM Encoder for Dialog Response Generation (tensorflow code) - https://github.com/dennybritz/chatbot-retrieval/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model then calculates the probability that the given response is the ground truth response given the context, by taking a weighted dot product: \n",
    "\n",
    "$p(r \\ is \\ correct|c,r,M) = σ(c^TMr+b)$ \n",
    "\n",
    "where $M$ is a matrix of learned parameters and $b$ is a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our knowledge, our application of neural network models to large-scale retrieval in dialogue systems is novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2016/04/Screen-Shot-2016-04-21-at-10.51.18-AM.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Generative Models\n",
    "* LSTM language model\n",
    "* HRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, we refer to a model as generative if it is able to <font color=\"red\">generate entirely new sentences</font> that are <font color=\"red\">unseen in the training set</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [9] 딥러닝을 이용한 자연어처리의 연구동향 - http://www.slideshare.net/ssuser06e0c5/ss-64417928\n",
    "* [10] 엘에스티엠 네트워크 이해하기 (Understanding LSTM Networks) - http://roboticist.tistory.com/m/571\n",
    "* [11] Generating Sequences With Recurrent Neural Networks - https://arxiv.org/pdf/1308.0850v5.pdf\n",
    "* [12] Generating Sequences With Recurrent Neural Networks (tensorflow code) - https://github.com/tensorflow/magenta/blob/master/magenta/reviews/summary_generation_sequences.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/random-160727015834/95/-25-638.jpg?cb=1469584793\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/images/post-covers/lstm.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [13] Building end-to-end dialogue systems using generative hierarchical neural network models - https://blog.acolyer.org/2016/07/01/building-end-to-end-dialogue-systems-using-generative-hierarchical-neural-network-models/\n",
    "* [14] Hierarchical Recurrent Encoder-Decoder code (HRED) for Query Suggestion (code) - https://github.com/sordonia/hred-qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we consider the Hierarchical Recurrent Encoder-Decoder (HRED) (Serban et al., 2015).\n",
    "\n",
    "The HRED model uses a hierarchy of encoders; \n",
    "* each utterance in the context passes through an ‘utterance-level’ en coder, and \n",
    "* the output of these encoders is passed through another ‘context-level’ encoder. \n",
    "\n",
    "This enables the handling of longer-term dependencies compared to a conventional Encoder-Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/06/hred-fig-1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Conclusions from an Incomplete Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Human Correlation Analysis\n",
    "* 5.1 Data collection\n",
    "* 5.2 Survey Results\n",
    "* 5.3 Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Survey Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Discussion\n",
    "* Constrained tasks\n",
    "* Incorporating multiple responses\n",
    "* Searching for suitable metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constrained tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporating multiple responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching for suitable metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Full scatter plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap18.png\" width=600 />\n",
    "<img src=\"figures/cap19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation - http://arxiv.org/pdf/1603.08023v1.pdf\n",
    "* [2] 'How NOT To Evaluate Your Dialogue System'Paper's slide - http://cs.mcgill.ca/~rlowe1/dialogue_evaluation_mila2016.pdf\n",
    "* [3] Laurent Charlin's homepage - http://www.cs.toronto.edu/~lcharlin/\n",
    "* [4] Chapter 8. Evaluation (Statistical Machine Translation) - http://www.statmt.org/book/slides/08-evaluation.pdf\n",
    "* [5] METEOR: Metric for Evaluation of Translation with Explicit Ordering An Automatic Metric for MT Evaluation with Improved Correlations with Human Judgments. - http://slideplayer.com/slide/7480005/\n",
    "* [6] Text summarization - http://www.slideshare.net/kareemhashem/text-summarization\n",
    "* [7] DEEP LEARNING FOR CHATBOTS, PART 2 – IMPLEMENTING A RETRIEVAL-BASED MODEL IN TENSORFLOW - http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/\n",
    "* [8] Dual LSTM Encoder for Dialog Response Generation (tensorflow code) - https://github.com/dennybritz/chatbot-retrieval/\n",
    "* [9] 딥러닝을 이용한 자연어처리의 연구동향 - http://www.slideshare.net/ssuser06e0c5/ss-64417928\n",
    "* [10] 엘에스티엠 네트워크 이해하기 (Understanding LSTM Networks) - http://roboticist.tistory.com/m/571\n",
    "* [11] Generating Sequences With Recurrent Neural Networks - https://arxiv.org/pdf/1308.0850v5.pdf\n",
    "* [12] Generating Sequences With Recurrent Neural Networks (tensorflow code) - https://github.com/tensorflow/magenta/blob/master/magenta/reviews/summary_generation_sequences.md\n",
    "* [13] Building end-to-end dialogue systems using generative hierarchical neural network models - https://blog.acolyer.org/2016/07/01/building-end-to-end-dialogue-systems-using-generative-hierarchical-neural-network-models/\n",
    "* [14] Hierarchical Recurrent Encoder-Decoder code (HRED) for Query Suggestion (code) - https://github.com/sordonia/hred-qs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
